{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {
    "id": "e26ea644"
   },
   "source": [
    "# Image Captioning with Transformers (Encoder–Decoder)\n",
    "\n",
    "An image-captioning model using a **pretrained ResNet50** encoder (image features) and a **custom Transformer decoder** for caption generation.\n",
    "\n",
    "What's included and *why*:\n",
    "- **Pretrained ResNet50**: gives strong visual features without training a vision model from scratch. Faster convergence and better captions.\n",
    "- **Patch/region features**: we convert CNN feature maps into a sequence the decoder can attend to.\n",
    "- **Transformer decoder**: autoregressive text generation with self-attention + cross-attention to image features.\n",
    "- **Beam search**: improved inference over greedy decoding; explores top-k candidate sequences.\n",
    "- **Evaluation**: BLEU (via `nltk`) and CIDEr (via `pycocoevalcap`)—CIDEr correlates better with human judgement for captioning.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {
    "id": "60e17b4f"
   },
   "source": [
    "# Setup & Install"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4995a13d",
    "outputId": "be06b758-b960-4fa3-b723-eea17706880b"
   },
   "outputs": [],
   "source": [
    "\n",
    "!pip install --upgrade pip\n",
    "!pip install transformers torchvision pycocotools tqdm nltk pycocoevalcap\n",
    "print(\"Install complete\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {
    "id": "07bbc5c5"
   },
   "source": [
    "# Imports & Device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c2dbd1d4",
    "outputId": "ea54160e-772c-484a-820b-553706a8f27f"
   },
   "outputs": [],
   "source": [
    "\n",
    "import os, math, random, json, collections\n",
    "import random\n",
    "from pathlib import Path\n",
    "from tqdm.notebook import tqdm, trange\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.datasets import CocoCaptions\n",
    "import torchvision.models as models\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device:\", device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {
    "id": "30fa07a3"
   },
   "source": [
    "# Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {
    "id": "16789f6d"
   },
   "outputs": [],
   "source": [
    "\n",
    "image_size = 128\n",
    "batch_size = 64\n",
    "learning_rate = 1e-4\n",
    "num_epochs = 50\n",
    "patch_size = 16\n",
    "hidden_size = 192\n",
    "num_layers = (4, 4)\n",
    "num_heads = 8\n",
    "\n",
    "sample_small_dataset = True\n",
    "max_train_samples = 2000\n",
    "max_val_samples = 500\n",
    "\n",
    "use_pretrained_resnet_encoder = True\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {
    "id": "9099988d"
   },
   "source": [
    "# Download COCO Captions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {
    "id": "22qVQ6Wct-Tp"
   },
   "source": [
    "The cell below supports a full download (set FULL_DOWNLOAD = True) or a demo path that downloads val2014 + annotations and samples a small subset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2f6a19b9",
    "outputId": "b94b5373-4d9e-428d-b88b-007a45973aab"
   },
   "outputs": [],
   "source": [
    "\n",
    "ROOT = Path(\"/content/coco_captions\"); ROOT.mkdir(parents=True, exist_ok=True)\n",
    "FULL_DOWNLOAD = False\n",
    "urls = {\n",
    "    \"train2014\": \"http://images.cocodataset.org/zips/train2014.zip\",\n",
    "    \"val2014\": \"http://images.cocodataset.org/zips/val2014.zip\",\n",
    "    \"annotations\": \"http://images.cocodataset.org/annotations/annotations_trainval2014.zip\"\n",
    "}\n",
    "if FULL_DOWNLOAD:\n",
    "    for k, url in urls.items():\n",
    "        out_zip = ROOT / f\"{k}.zip\"\n",
    "        if not out_zip.exists():\n",
    "            print(f\"Downloading {k} ...\")\n",
    "            !wget -q -c {url} -O \"{out_zip}\"\n",
    "    for k in urls:\n",
    "        out_zip = ROOT / f\"{k}.zip\"\n",
    "        if out_zip.exists():\n",
    "            !unzip -q -o \"{out_zip}\" -d \"{ROOT}\"\n",
    "else:\n",
    "    if not (ROOT/\"val2014.zip\").exists():\n",
    "        print(\"Downloading val2014.zip (≈6GB)...\")\n",
    "        !wget -q -c {urls['val2014']} -O \"{ROOT/'val2014.zip'}\"\n",
    "    !unzip -q -o \"{ROOT/'val2014.zip'}\" -d \"{ROOT}\"\n",
    "    if not (ROOT/\"annotations_trainval2014.zip\").exists():\n",
    "        print(\"Downloading annotations (≈241MB)...\")\n",
    "        !wget -q -c {urls['annotations']} -O \"{ROOT/'annotations_trainval2014.zip'}\"\n",
    "    !unzip -q -o \"{ROOT/'annotations_trainval2014.zip'}\" -d \"{ROOT}\"\n",
    "print(\"Data in:\", ROOT)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {
    "id": "23348b92"
   },
   "source": [
    "# Data Transforms, Dataset & Dataloaders"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {
    "id": "rjU1JOGVuM_4"
   },
   "source": [
    "- Resize to `IMAGE_SIZE` because ResNet expects ~224×224.\n",
    "- `RandomCrop` + `flips` on training improves generalization.\n",
    "- Using `SampleCaption` to randomly choose one of the 5 COCO captions per image — this provides diversity without duplicating images in the loader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 471
    },
    "id": "16fa49e3",
    "outputId": "f14170b7-bf4a-46ed-a47a-547b75464aae"
   },
   "outputs": [],
   "source": [
    "\n",
    "class SampleCaption(object):\n",
    "    def __call__(self, captions): return random.choice(captions)\n",
    "\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize((image_size, image_size)),\n",
    "    transforms.RandomCrop(image_size, padding=4, pad_if_needed=True),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225]),\n",
    "])\n",
    "val_transform = transforms.Compose([\n",
    "    transforms.Resize((image_size, image_size)),\n",
    "    transforms.CenterCrop(image_size),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225]),\n",
    "])\n",
    "\n",
    "ROOT = Path(\"/content/coco_captions\")\n",
    "train_images = ROOT / \"train2014\"\n",
    "val_images = ROOT / \"val2014\"\n",
    "ann_dir = ROOT / \"annotations\"\n",
    "train_ann = ann_dir / \"captions_train2014.json\"\n",
    "val_ann = ann_dir / \"captions_val2014.json\"\n",
    "\n",
    "if train_ann.exists() and train_images.exists():\n",
    "    train_ds = CocoCaptions(root=str(train_images), annFile=str(train_ann), transform=train_transform, target_transform=SampleCaption())\n",
    "else:\n",
    "    print(\"Train set not found. Using val set as training for demo.\")\n",
    "    train_ds = CocoCaptions(root=str(val_images), annFile=str(val_ann), transform=train_transform, target_transform=SampleCaption())\n",
    "\n",
    "val_ds = CocoCaptions(root=str(val_images), annFile=str(val_ann), transform=val_transform, target_transform=SampleCaption())\n",
    "\n",
    "if sample_small_dataset:\n",
    "    rng = torch.Generator().manual_seed(42)\n",
    "    train_n = min(len(train_ds), max_train_samples)\n",
    "    val_n = min(len(val_ds), max_val_samples)\n",
    "    train_ds, _ = torch.utils.data.random_split(train_ds, [train_n, max(0, len(train_ds)-train_n)], generator=rng)\n",
    "    val_ds, _ = torch.utils.data.random_split(val_ds, [val_n, max(0, len(val_ds)-val_n)], generator=rng)\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True, num_workers=2, pin_memory=True)\n",
    "val_loader = DataLoader(val_ds, batch_size=batch_size, shuffle=False, num_workers=2, pin_memory=True)\n",
    "\n",
    "print(\"Train samples:\", len(train_ds), \"Val samples:\", len(val_ds))\n",
    "\n",
    "imgs, caps = next(iter(val_loader))\n",
    "plt.figure(figsize=(3,3))\n",
    "out = torchvision.utils.make_grid(imgs[0:1], 1, normalize=True)\n",
    "plt.imshow(out.numpy().transpose(1,2,0)); plt.axis('off')\n",
    "print(\"Sample caption:\", caps[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {
    "id": "b522e282"
   },
   "source": [
    "# Tokenizer + TokenDrop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {
    "id": "o5W-CRpOuqF3"
   },
   "source": [
    "Using `distilbert-base-uncased` tokenizer to convert captions to token ids. The decoder's final logits will predict over this tokenizer's vocabulary.\n",
    "\n",
    "Tokenize captions on the fly to avoid storing a full tokenized dataset. `TokenDrop` randomly replaces tokens with pad token to regularize decoder reliance on previous ground-truth tokens (reduces teacher-forcing memorization)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 274,
     "referenced_widgets": [
      "5de37566d3c94f95b8a6eeed02f2f363",
      "5f98b1ee2a72466eb41fd930a9d54b0d",
      "b3a5ac1bb64a4acea0dc4422fab03e8e",
      "fc7c6f6d669a4fa7a28ef63d38cc04fb",
      "254a24977f08427c8dea1e4723ae7402",
      "dbc5e8db670b4b2e9ef7317b8154b04d",
      "e2ac1e8c8d2c4b36a93f8608b75616f5",
      "05e48748d6e0485ab0ea3de1bbc15ba9",
      "6e1d916c162746989d13edcb15edb86d",
      "cb445400d48a4856b9fc241139ca4a44",
      "3d123405eae8446bb3b2c0162f99567c",
      "8734ab40e2574ca3b07834c9cbf29e70",
      "e651958e6209469fbbbddf6d29c0d48d",
      "f6d8168f5b6d45629fd24b67b8932ee6",
      "a382f1e58adf4f0e9173e69890146e6a",
      "cc1abf41614c4389aa08b637a43b60fe",
      "d466da3f41b7426785c58a0f1e0ae349",
      "184a8f8204b7459684b5688032831e5d",
      "49a8fe31ba4e45b8bd304ae3605fb8e4",
      "083870a1499943b6992d47f3f865dc57",
      "95939042193e4dd4bdc10d56a8a3702d",
      "987db3d2136c4e79a278aa6297dd6a0c",
      "f54143efbad448339ab5931f239de92d",
      "dcf3b7370e1449b496016aaa1725d696",
      "6b4c72166d7548cc9bde4b0e28425902",
      "09e965296dcf476095f47aa461a91c6f",
      "46110d5dcc464e008c34a7dc80099553",
      "6ab29bf70d154e80a2877560175479b0",
      "33c61fe6da5442e1a82362f9bd6e38e5",
      "c34ee850b7a741bf8aad5bc596724f68",
      "b5e42a64c4c14d68a5807cf649ce29e5",
      "bbac386c250d451a940ea445ca7c6d84",
      "9b7954d5705d45549656e00c0715550c",
      "33cc7efbc98f45e79245ca3fa821b0cc",
      "cd32e3ef58fc4a55b69ad65e62d51e88",
      "8f1a22d65b58431b839e9889c36f7941",
      "739ac7bdb1cf483ab839496dd098fa95",
      "263159035aaf449a90b5add895c705eb",
      "bb2d606e21344d05a14ab79e63761207",
      "dfb207602ebf4a05a5430467894be0db",
      "43aa4f720b1442129fb009f05999f286",
      "a7a2bd1203784095812960779b33b43f",
      "0700d8c9d71c40569325c9b637e7a982",
      "2977137ddaa34fe69fcefd4450dba189"
     ]
    },
    "id": "a2dac04e",
    "outputId": "a5416e25-e149-4336-ceb3-69dc90c7b9d8"
   },
   "outputs": [],
   "source": [
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "print(\"Vocab size:\", tokenizer.vocab_size)\n",
    "\n",
    "class TokenDrop(object):\n",
    "    def __init__(self, prob=0.5, blank_token=None, eos_token=None):\n",
    "        self.prob = prob\n",
    "        self.blank_token = blank_token if blank_token is not None else tokenizer.pad_token_id or tokenizer.mask_token_id\n",
    "        self.eos_token = eos_token if eos_token is not None else tokenizer.sep_token_id or tokenizer.eos_token_id\n",
    "        self.cls_token = tokenizer.cls_token_id\n",
    "    def __call__(self, input_ids):\n",
    "        mask = torch.bernoulli(self.prob * torch.ones_like(input_ids)).bool()\n",
    "        if self.eos_token is not None:\n",
    "            mask &= (input_ids != self.eos_token)\n",
    "        if self.cls_token is not None:\n",
    "            mask &= (input_ids != self.cls_token)\n",
    "        out = input_ids.clone()\n",
    "        out[mask] = self.blank_token\n",
    "        return out\n",
    "\n",
    "td = TokenDrop(prob=0.5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16",
   "metadata": {
    "id": "3a7dfda6"
   },
   "source": [
    "# Model – Encoder–Decoder Transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17",
   "metadata": {
    "id": "lJf3zJCQvQsB"
   },
   "source": [
    "- **Why use ResNet50?** Pretrained ResNet50 yields high-quality spatial feature maps. Remove the final classification head and extract an intermediate feature map (e.g., after layer4) which has spatial dimensions (H', W'). Flatten spatial positions into a sequence the decoder can attend to.\n",
    "\n",
    "- Decoder receives **token embeddings** + **sinusoidal positional embeddings**.\n",
    "- Each block does self-attention (causal mask) then cross-attention to image features, then MLP.\n",
    "- Output logits map to the tokenizer vocabulary.\n",
    "\n",
    "This approach is simple and effective: the decoder learns to attend to different spatial regions when generating each token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {
    "id": "8481e417"
   },
   "outputs": [],
   "source": [
    "\n",
    "def extract_patches(image_tensor, patch_size=16):\n",
    "    bs, c, h, w = image_tensor.size()\n",
    "    unfold = torch.nn.Unfold(kernel_size=patch_size, stride=patch_size)\n",
    "    unfolded = unfold(image_tensor).transpose(1,2).reshape(bs, -1, c*patch_size*patch_size)\n",
    "    return unfolded\n",
    "\n",
    "class SinusoidalPosEmb(nn.Module):\n",
    "    def __init__(self, dim): super().__init__(); self.dim = dim\n",
    "    def forward(self, x):\n",
    "        device = x.device; half = self.dim // 2\n",
    "        emb = math.log(10000)/(half-1); emb = torch.exp(torch.arange(half, device=device)*-emb)\n",
    "        emb = x[:,None]*emb[None,:]\n",
    "        return torch.cat((emb.sin(), emb.cos()), dim=-1)\n",
    "\n",
    "class AttentionBlock(nn.Module):\n",
    "    def __init__(self, hidden_size=128, num_heads=4, masking=True):\n",
    "        super().__init__(); self.masking = masking\n",
    "        self.mha = nn.MultiheadAttention(hidden_size, num_heads=num_heads, batch_first=True, dropout=0.0)\n",
    "    def forward(self, q_in, kv_in, key_mask=None):\n",
    "        attn_mask = None\n",
    "        if self.masking:\n",
    "            L = q_in.shape[1]\n",
    "            attn_mask = torch.triu(torch.ones(L, L, device=q_in.device), 1).bool()\n",
    "        return self.mha(q_in, kv_in, kv_in, attn_mask=attn_mask, key_padding_mask=key_mask)[0]\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, hidden_size=128, num_heads=4, decoder=False, masking=True):\n",
    "        super().__init__(); self.decoder = decoder\n",
    "        self.norm1 = nn.LayerNorm(hidden_size)\n",
    "        self.attn1 = AttentionBlock(hidden_size, num_heads, masking=masking)\n",
    "        if self.decoder:\n",
    "            self.norm2 = nn.LayerNorm(hidden_size)\n",
    "            self.attn2 = AttentionBlock(hidden_size, num_heads, masking=False)\n",
    "        self.norm_mlp = nn.LayerNorm(hidden_size)\n",
    "        self.mlp = nn.Sequential(nn.Linear(hidden_size, hidden_size*4), nn.ELU(), nn.Linear(hidden_size*4, hidden_size))\n",
    "    def forward(self, x, input_key_mask=None, cross_key_mask=None, kv_cross=None):\n",
    "        x = self.attn1(x, x, key_mask=input_key_mask) + x; x = self.norm1(x)\n",
    "        if self.decoder:\n",
    "            x = self.attn2(x, kv_cross, key_mask=cross_key_mask) + x; x = self.norm2(x)\n",
    "        x = self.mlp(x) + x; return self.norm_mlp(x)\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, num_emb, hidden_size=128, num_layers=3, num_heads=4, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(num_emb, hidden_size)\n",
    "        with torch.no_grad():\n",
    "            self.embedding.weight.mul_(0.001)\n",
    "        self.pos_emb = SinusoidalPosEmb(hidden_size)\n",
    "        self.blocks = nn.ModuleList([\n",
    "            TransformerBlock(hidden_size, num_heads, decoder=True) for _ in range(num_layers)\n",
    "        ])\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc_out = nn.Linear(hidden_size, num_emb)\n",
    "\n",
    "    def forward(self, input_seq, encoder_output, input_padding_mask=None, encoder_padding_mask=None):\n",
    "        x = self.embedding(input_seq)\n",
    "        B, L, H = x.shape\n",
    "        pos = self.pos_emb(torch.arange(L, device=input_seq.device)).view(1, L, H).expand(B, L, H)\n",
    "        x = self.dropout(x + pos)\n",
    "        for blk in self.blocks:\n",
    "            x = blk(x, input_key_mask=input_padding_mask, cross_key_mask=encoder_padding_mask, kv_cross=encoder_output)\n",
    "        return self.fc_out(self.dropout(x))\n",
    "\n",
    "\n",
    "class VisionEncoder(nn.Module):\n",
    "    def __init__(self, image_size, channels_in, patch_size=16, hidden_size=128, num_layers=3, num_heads=4):\n",
    "        super().__init__()\n",
    "        self.patch_size = patch_size\n",
    "        self.fc_in = nn.Linear(channels_in * patch_size * patch_size, hidden_size)\n",
    "        seq_length = (image_size // patch_size) ** 2\n",
    "        self.pos_embedding = nn.Parameter(torch.empty(1, seq_length, hidden_size).normal_(std=0.02))\n",
    "        self.blocks = nn.ModuleList([TransformerBlock(hidden_size, num_heads, decoder=False, masking=False) for _ in range(num_layers)])\n",
    "    def forward(self, image):\n",
    "        x = self.fc_in(extract_patches(image, self.patch_size)) + self.pos_embedding\n",
    "        for blk in self.blocks: x = blk(x)\n",
    "        return x\n",
    "\n",
    "class ResNetEncoder(nn.Module):\n",
    "    def __init__(self, hidden_size=128):\n",
    "        super().__init__()\n",
    "        base = models.resnet50(weights=models.ResNet50_Weights.IMAGENET1K_V2)\n",
    "        modules = list(base.children())[:-2]\n",
    "        self.backbone = nn.Sequential(*modules)\n",
    "        self._out_channels = 2048\n",
    "        self.proj = nn.Linear(self._out_channels, hidden_size)\n",
    "    def forward(self, image):\n",
    "        feat = self.backbone(image)   # [B,2048,Hf,Wf]\n",
    "        B,C,Hf,Wf = feat.shape\n",
    "        seq = feat.permute(0,2,3,1).reshape(B, Hf*Wf, C)\n",
    "        return self.proj(seq)\n",
    "\n",
    "class VisionEncoderDecoder(nn.Module):\n",
    "    def __init__(self, image_size, channels_in, num_emb, patch_size=16, hidden_size=128, num_layers=(3,3), num_heads=4, use_pretrained=False):\n",
    "        super().__init__()\n",
    "        if use_pretrained:\n",
    "            self.encoder = ResNetEncoder(hidden_size=hidden_size)\n",
    "        else:\n",
    "            self.encoder = VisionEncoder(image_size=image_size, channels_in=channels_in, patch_size=patch_size, hidden_size=hidden_size, num_layers=num_layers[0], num_heads=num_heads)\n",
    "        self.decoder = Decoder(num_emb=num_emb, hidden_size=hidden_size, num_layers=num_layers[1], num_heads=num_heads)\n",
    "    def forward(self, input_image, target_seq, padding_mask):\n",
    "        bool_mask = padding_mask == 0\n",
    "        enc = self.encoder(input_image)\n",
    "        dec = self.decoder(input_seq=target_seq, encoder_output=enc, input_padding_mask=bool_mask)\n",
    "        return dec\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19",
   "metadata": {
    "id": "1aa03586"
   },
   "source": [
    "# Initialize Model & Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a8b6e63f",
    "outputId": "7c348841-1eff-44ac-875e-2713dbc76672"
   },
   "outputs": [],
   "source": [
    "\n",
    "try:\n",
    "    sample_images, _ = next(iter(train_loader))\n",
    "    channels_in = sample_images.shape[1]\n",
    "except Exception as e:\n",
    "    channels_in = 3\n",
    "\n",
    "caption_model = VisionEncoderDecoder(\n",
    "    image_size=image_size,\n",
    "    channels_in=channels_in,\n",
    "    num_emb=tokenizer.vocab_size,\n",
    "    patch_size=patch_size,\n",
    "    hidden_size=hidden_size,\n",
    "    num_layers=num_layers,\n",
    "    num_heads=num_heads,\n",
    "    use_pretrained=use_pretrained_resnet_encoder\n",
    ").to(device)\n",
    "\n",
    "optimizer = optim.Adam(caption_model.parameters(), lr=learning_rate)\n",
    "scaler = torch.cuda.amp.GradScaler() if device.type == 'cuda' else None\n",
    "loss_fn = nn.CrossEntropyLoss(reduction='none', label_smoothing=0.1)\n",
    "\n",
    "num_params = sum(p.numel() for p in caption_model.parameters())\n",
    "print(f\"Params: {num_params:,} (~{num_params/1e6:.2f}M)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21",
   "metadata": {
    "id": "d5413885"
   },
   "source": [
    "# Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {
    "id": "l5YNeQ9ifAlP"
   },
   "outputs": [],
   "source": [
    "num_epochs = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 977,
     "referenced_widgets": [
      "7ee34587dc3248de8f1707e6da4bc1eb",
      "3f79aacb7e9e41d5ba759775004a16e9",
      "514c49d8e33042b6adc9dfee799cdd5b",
      "b5db3e4a74c64a7ea1dbc853908a5622",
      "32b9a97b60dd4fbd892abfc4e74d2c9f",
      "8077dcd9e7204883a053c8d370b9336c",
      "1a9096897d2344468a68948d644c2bf8",
      "a114b10b9e494ea3b5d5c58396570670",
      "29154206ed294e7b87a2597e2f5fb604",
      "9de31344f5524604a11509c7540abe7d",
      "bbb53d3fe33e43c8889a815c29ad07e3",
      "21b43b4094184dc1af46e71fc0e5f734",
      "0e4564509d8b464bb3ed858d891dba5a",
      "214f3931482b4a4d80a764529a7d7591",
      "cabd5eef35d74771978225bfe368a63b",
      "a167ac13e99d49c08e6613ecf8da24a1",
      "0b24d76f6e9445108e76d11d114260fb",
      "9391100fb3fc4d2595de9179da9c0bc6",
      "163ca30427884bef9de231ed19e8c192",
      "156e8daf6766442c8651b7ea18b6d008",
      "f52f9c3b0dc347038cc066fabbdd6e2a",
      "98e5990f6bbc48db84bc6a275f552993",
      "c3ab46f64fb148ccbac44ff0856e1d93",
      "5e95ddb64edc49b5b287ad437e4d36af",
      "48592ca60f414e44bd005fe84222f3a6",
      "0db1c1efdcdb4ffcbd17be3e89dac7cc",
      "ef53b20fd9e648d9b21da3d474f25f30",
      "47e0d14796b048d096a92a0a6bac4a27",
      "3d124cdf262d445cb4167ba6eb6c1e1a",
      "5c1c1fdf345d4fcd9b2cd6d3b925d661",
      "15852c4f2c134ba5a87d91564e470db2",
      "a6ed5b80a5ca41c5ae43944670558a10",
      "71e6425698894e9b8604a06a9002bb29",
      "11a02f4e66264d66b54327d7937e817f",
      "0da831b6788449f7a7cf898bbd4a413f",
      "c7e9736705f64028baf8f13483c4d1c3",
      "24c595873c70482c995e55b689865406",
      "3b7292c2386b4e338830d4ca79526fea",
      "d6a3d6f6547e4d6e870fc75bbec94f7b",
      "906b44fa7a6a4c3a8e34f56954a69017",
      "bede915bf3b440279c71d439585ff567",
      "db788d97f30a4190a364d0cdda375f29",
      "7b4d1d2b4c934549bc5975f7f6125203",
      "5427ffbbabd74ac2855f7e2c538a690a",
      "3fc9ae7307074b6790f2e7aea2663209",
      "ac464689799a4a999ab68836556fc725",
      "b4cd82f49759432fafe1e7908bcaa0f1",
      "fabaaee8e3cd4ba88f4942557bfc69e3",
      "ae5a7a8d9c6d4cefab5e6de6a26143c3",
      "f5b44550d2e9494c95c64f0166aaf530",
      "1f9590ff9f5d4383bdeee1940c6d8022",
      "8b91108aa8ba4941b9c20d493bd21594",
      "b5f99c9e5b804de5b4e9680d8783601e",
      "04eeb82f4e8f48bb9b87c6a4b0d71d2e",
      "5a7eafa056c04bf398ec6129efc17b35",
      "01599da26ebe477da171aa48023996e5",
      "de2e6279ec9f455ba9b5ab7028b8d3df",
      "b626e650062542b78a4a46ee65336f62",
      "5b33bf07a4b343969edb1473f2730360",
      "97f374db419d4c2caa474f339cece7da",
      "f3103e26299049acb1dc5d861de097d8",
      "d84acd2bdac849aa919c203e8621defb",
      "0f56eb17d9594a58bc183049d5dde6a6",
      "6684f12dfd2e401aa7412aa20e02139e",
      "499b631b00404417bc5cb1b48ecdf052",
      "6544200088ab4bb88ff4de659bcf7092",
      "69d39048561a4fc49148b701fd9ab9fc",
      "0b7cee0035844b139ecf6c59d97463b8",
      "041dddd80e044761be7f1f9ae655bb9d",
      "c1aeadb7c22a4fe2903c14af4d9364fb",
      "9b7278a74d844cb79dae8e90b40e680d",
      "6d78d9ca53d648b2ab2134bd16371e21",
      "dd140ee3b4e44d1cab10efe1cb601c58",
      "56c447c18b6849999bd3a8fc4c083437",
      "fdd97ea2d7e84b71a9149a77c5b2992a",
      "9ff0ff01545f43e78b6c2ab496d37bb4",
      "a70f16a3433d4e2dafe73091092dd74d",
      "5b4acddb0a3e4687a8d7a0e81256344e",
      "ab0fa541dec243efbd7e002b98f141b5",
      "5929318d076b4eaf82fe33bf269825a6",
      "5bf356e3ad034e00b8e66a337fa2286e",
      "1e3917233f9d45bca3e14be69f20402f",
      "aa0ee3df1843447385748900b42f6d45",
      "f652174d0cbd4139a942c1af400cf1af",
      "99962b7b376e44948190fe020f5ad322",
      "e2738ccce8ce43c88f6f0abcd5333d88",
      "684a8327677e4d0dbf58e2d3c3aded45",
      "8acb42331f8f45a7872c38717ee43df5",
      "5b8803c4694a4fdc81f619e4220cca17",
      "62d79849d383404789404496d9a22e2b",
      "3fe0312c89d447aaade1da8ff3a59eb4",
      "ee2bac1641f448dfaa44e767395496d7",
      "bb68f4ea6ea64ac0b5090ecf2084ab32",
      "bdb387ea5bbf42619e264d0a218233af",
      "6b4d5e14c88c4b2083b617f630287103",
      "5b1d97a54541440ba26cc99c521c0247",
      "b9fe0f404c8b4e1dae8ae15a3298b855",
      "0740276f3b774e2593013f507968ec34",
      "02bd1e706f4d43f5a8c19edb425902a4",
      "59e86923f31b400c8173f5b135d63cf8",
      "23fd3ca08e1e4de3ae83a5b5976734f6",
      "a2623e7399ba4ff19ce862184542feba",
      "143bbec5f98046beba5382881a91424b",
      "ded0487a76b24d31bc064f89fa6e4220",
      "30b8e8e77c7942c7b1f811386a1d27f1",
      "6930e0fb00194129bb127c0a2ee8cd75",
      "56c2b68b7be240c0ac42fe0976f2e160",
      "f585339e1a5e4fbcbe838983a56e88d8",
      "68981fd2e13d4b70bc64dfe3488884f5",
      "0f28f9131cc74b5f8241555036f175a6",
      "405f0a3d09f3496eb9b284ef2e4a6fd9",
      "33c5d54a96614e5898ec4caf42780201",
      "cbd1cc8dee894b17905f98def5be4335",
      "3b09b76928e14394be2273e4d7e57555",
      "3ab376597c304bd99c5904276eebd3b0",
      "885e85d0a6e84e4cb9de27b1c816deb0",
      "12d36a3965714fd8bced92ae3d1a5d11",
      "c8e0075927bb4f45995aa63bb2054a69",
      "0051be3621084954ab81e6ac347c5738",
      "9b762122286040d4921c9c0c2eabc4e2",
      "3b5ff7c09a48453f88a5f4e0f252ae8e",
      "cf062eac2cea4e60a86abaabc538834f",
      "378477ca982c44218ef9444876333ace",
      "257d74b251794826aadada3f38e58e7e",
      "aa7ca6979389409097c8a2fd1371bcaf",
      "53cf90cb4ab44b55931ef44150e40692",
      "151a68886e1040a5a9c4c9d35d1803c2",
      "f5c526dd3279408380700ac60131ae55",
      "74c62c10d7594692bca64c0564b6f98c",
      "0499c51a0e224a7ca9f19e9a79552aab",
      "06e811beb55447de8be8363fc9e9e30e",
      "b18ee6d6524a457b8e990508f799b2b0",
      "659c24e00dae4e709f0b58a1cb0d8c00",
      "17cae2e1a59144fdbbbe5b8931f2c1bf",
      "1909bd7457e049f885229e511adfae93",
      "77254ad69bb247b78a471b73dfdf2741",
      "b7977767ec534446a6d6d21ef97ecb2f",
      "06d4583cb62e420f98527d644e60229f",
      "4ddab395ac434c76aed8dc0e3c725415",
      "238fb41fb2164b0b9d4cdbd9722fd10a",
      "d3b566b0c55b427ba87ec41fd410c1e0",
      "a6c2b90fbc4942a4944710e560bef30e",
      "98b5070d3d534e90af8683ca42d25c87",
      "dede5e836b1444dab07725642a1a598e",
      "07f2f3c55dfd4c02a23b133d4680b562",
      "f74c05ad06be49309b78ba2d880b7ebd",
      "8c3a3ffb402248debba1455fdbbbd577",
      "22fde5b0db0e46beba668ea59b40618e",
      "5ac801140e2a415d90abaa3c51d77642",
      "4ca5e0141595420eab4795d8f7054989",
      "10790280ee044fd6b6f3310919bf99fa",
      "bca20d39bd474bbaa993b0716c86acb0",
      "ad813efdacc746659e10febc579f28fb",
      "e83af3453c1142739f9c6073411fbaa2",
      "7df0b8b5c2934bb89cca528faf96a8bb",
      "fbeb009c967f4de1b13b98e356c75190",
      "a8eb5baddf7649b3a8cefe4aef8f25ec",
      "d1b7173011ea4faf93a1dc1c173d1ed1",
      "7109e525b42b45daa2b52db5f26129cf",
      "927ff2d592334d5f8cafcaaa8c317f2b",
      "21a6dc788fb847b7a9433f7e9f4c9546",
      "59f2ec9f47544647b431f56ebfbdd7b9",
      "e59b5316f3744b339b12f7ab21bcd839",
      "43364f6b262e479c930a0b3178617fc7",
      "6c539e8343124379841eb65f2dd40283",
      "864c034ccea845d888c7e2a8202ace0a",
      "cfd58c1faf21476a83b47a17c345193a",
      "3c7c43f200684f698ec10985c3379a49",
      "43f7a9849b8b4324b04fdaa4fa5ed855",
      "4a95b8ad14924f708d81e5478ebb1531",
      "f5e5857c765447c3bb250be525408701",
      "e9b0874847a74c27a3ee857337e085e2",
      "78818dca0480487a93652f94ae5db1da",
      "f95362f8c86d4d3082c42a3cfca8d961",
      "762fd62533714db9adc1368992a312a7",
      "990fe71a7ca2485a905cc5750c04915b",
      "aecdc2f34b144bd8bcfa15d5ab20bd28",
      "4d01533c7fa546dea23604d15e7e9b54",
      "99714bedea8044e08c02bda84106e50a",
      "ce110c463a6e476abc5653f02d80b746",
      "61c935ff1bfe464d8bdcb2a0a39c09ab",
      "1877ad8b38004582aebbe19745c42952",
      "66b09cf7e601485ea92a7b576a98ec6c",
      "c2d4e1a9b9f246baa6cc0fd4dda1899d",
      "ea28d4117514410e9bb06574d20140be",
      "2047039e2e1e430299dec982b03413b8",
      "d185e8eccee34f709bccb157eacd28f8",
      "e905085ced2f4605b5bd2ee045d011b1",
      "19d18f40189240dc9eb28097b2b7c157",
      "2a961aa3412c41e0a1646b053c9c6fe4",
      "3a5e7cd1b1ea47d5bb2b669a7d53dffc",
      "5ac76b1a021f45c0bfba73e99d0e9d10",
      "4a6e20ba0eb449d48ecac4dc70bd5377",
      "3f2d9670d7384f4c855a09b8314692cc",
      "497497df67434be5b95bbe31eacd26bf",
      "5604ed0eeb1144d1a19914c5f25b0082",
      "7aa589c9e862414590cfcae3006c91fb",
      "757a209a69a4424cb394f4a72ceff1d9",
      "544e151f26354aad8d499b4dfc2ba4f3",
      "60228795c7de4447b6e0bc40fd5be6d7",
      "6001d21a3e2b4811966c43ea24b74749",
      "b12eb926036e4f7a97c3af1fddfef514",
      "8474943de5884b56ae0d35a2f73263a6",
      "33a69804b8e84abe9b7aaabbdc9bef24",
      "c5de8754e3cb47e4a3147bdceefed29c",
      "5d33e973eb6649b58833151f46a20348",
      "7b9fd7d8eab14bd082c4a6b86734ce5e",
      "7cde501dd47b4baaa138c30f1d531dff",
      "0737c92d62394ac5aa481ff990761b5d",
      "b96cbd3b691a424db4613ce92975be08",
      "f8ee6d85f6b94713a396849a9e002a89",
      "edafa1f5d4d54d7d9ddec1a9f9510947",
      "e24e542e08a84696913e249a2d004630",
      "c95854eefae543b38588db6fcb2f9370",
      "2c11add1d0bf4b1fae2cf3b3ef125989",
      "3fb3c31659fe46ed8f79600c275d4cff",
      "32cca901978d4d84816caf92eee4b1f9",
      "2a569fd57b4e42f9957f1eb64557bb52",
      "67219c2bef614089844ca101aa8cec54",
      "bd253287b6ec416fb5a1758c2b5a25f8",
      "6a221092c74a47aa82ce67d12ca32172",
      "048f7e4469cf499bbecda8cb1a0d9972",
      "c8e554b06c54403993001f60449ce8fc",
      "90fce13661284d898b9076647f786810",
      "15753428894a424e9e4c3bcbe88239cc",
      "415ff92f078143eb9cae3c0bc8de35ad",
      "ec087fd7bac446f684c574c8625d8cbb",
      "65e43a023b6e409f83e5e5a4439363d3",
      "c3031a1560074af883bbd3bbc4deed32",
      "9e62edb0948e45d0b8692de3ecd677b3",
      "d5b8da9806354e2da19e41553298f34b",
      "ca07ea61240b4caaa069116acecb270d",
      "b7ad92ac81bd4d11a7d82b23c8dd172b",
      "7680f48e1b6c4d1e9e3af37103cd4da4",
      "f4c7567fca9242a7ae7ea3d8e8c4ec2e",
      "823c274bdc6445768b45bec59ae49291",
      "b865fc464fc645559126eb4ec6c2dd9e",
      "60c84c5ee5924716bf98b2245744ffc3",
      "e1ec67b443fb42a8a8af9ef0ee49250e",
      "d710a87f7e0b4d8f97dd3aada4845f06",
      "f31168c0a42b4a989586a46160529e09",
      "f2c03f0a526a4edb896b50a86836f67b",
      "951249475aeb44f2b1fb7b1df0b2cbea",
      "5226f8495d5b4d9f80937be784c6f6b2",
      "130ec67211494a0bb2f27e22424bacc6",
      "130fe424dfc04913be93f8f4e10d0a2a",
      "d40b75a971b34d65884ab74a32c7b993",
      "6631486fdf924abf97f810bea088a580",
      "01eb9d2dd94f43d090ddb71225506078",
      "6fa1e94a812c4655a90d9a1b13ef94d4",
      "22e602e33c8e44528a05299ce89f316d",
      "10e95e41a092470fb6c496bb19ed17cd",
      "c84639d8a4ac4e4cace01854f268a171",
      "18590b12d910477cab2c0ae60be66d34",
      "131dca232473404585a002bf1af52d79",
      "9a083fe0f00349a2b506ff0e83e6f522",
      "524bcf1f6ec94dc2a811617a1b9d6bac",
      "317b5845641c4df5897168c840bc512e",
      "49d5df90ce154199a05068ee15bfdcfe",
      "654c9f5fa70a45b880c59c70bf0a5cf9",
      "9b2be290a22d435b9f263284ab37e7e9",
      "7a9ca88de5ed420eb788b6d173639d21",
      "2bffc45af5fb4034b3c463e3d7d49c3a",
      "e2f7005829124be48a779bd478e9d54c",
      "44a5392e57fd4c8596ed38f139833c64",
      "d57dbe26ca664b8987d2a4b66db355be",
      "f6eb31efe6274a799dae0a3c877cdba9",
      "e54bf4f28f294cdfb9a46e98d8d268e9",
      "71d610f95ec84ac5bed2f3c958315084",
      "139b67ff38ef459d87f262635a3c3e8e",
      "ff45254e405944a591858185fb663978",
      "c8eccfcdc065438797ae977969b6f644",
      "7ebc84dc2b8842ab903a39cde534abf9",
      "a095feb504244216b0234c2e46514a2a",
      "1d117a85e1fd47b4a235ea6edd31c7fd",
      "08eb075d941c40b3ab23f8349266d3e3",
      "12e04492d1b94b07899d2e91092065a2",
      "a96dad244ce64734ad874e2a35968446",
      "d7b924fd8fbd40b29bd07483895ff0a9",
      "f68b238d630843d0b1cdb786783fc959",
      "c7f2eff54f3540d9a7bb405db2b76663",
      "db7c64031c5f45adbe7c92b27e79ba16",
      "3d3e02913b9f44c380646522748e31c2",
      "68822d18a4a842e49490922acd7df82c",
      "08d6be20c9764ac68f6adead5e85bcd4",
      "29d804eb80dd432ea9fdcb788c0aa59d",
      "3482fb8d231a45b691cbe570b54e74f4",
      "b2cae1351dc64749b61deb2f13ac1ed9",
      "4d654b68bc994c27a5d73613645e3da3",
      "04f86b1184204e96affff30a376d1872",
      "5ff96a267b6c4ce7a934f0176ff1471d",
      "d630dd963c074eefb30d3d58f1ea896a",
      "0918b93b68bc4a7aa3d052680f7818d6",
      "7fac485f21df415982976c674469231c",
      "207a71eb912f46c9aca396bb463d53c7",
      "2e8498f5fe0d45348bf6768faabbc482",
      "27edc16e096c4069a19acb0c01aed412",
      "ca3fcf7a0c11484e8314b90ea147698a",
      "da841117599242779cb3f1a1f226fd6a",
      "d9ff139206c946298ce7d72dcb4dc339",
      "4ddb5f89ad0e425b9d4fb73db412b663",
      "a50e7e95d63147d596e491ec0d6578c8",
      "374444d56bed4222a660a07c8ebcbaac",
      "8d169d7c599f4e1cab11b9e24d7cbcd1",
      "b757d60847bc4ad7be86089fa7b96f2a",
      "27f89d5493ed4f6595eca391a9c4151e",
      "b356b2771e444acfb0938a0016eecb3f",
      "b0d35cec88e1467d9ccbe323c0e158f0",
      "e721125b628949d8b3446acdb0890cc5",
      "4fd7732f162c4e2097755717c3536367",
      "f9dafa9a8032430a8fe2bea524a07fb7",
      "6cfcf632853f4dc8a9ccbdf31ddac775",
      "ded34fc4d27c4a53a059eed3e797c8b6",
      "ed6c2ce0d8f34d87bea1be0658a1bbc4",
      "5adeb0f041ae41a880a6b5bd0ac9d7b9",
      "465b94933ca143e1933e03a344e5e232",
      "6e1a7966b8334c07893ab4cc45c00977",
      "734a5c62d3614d1082f690b0c990d23c",
      "a1da358bea4842e58c23adce3b89474c",
      "23bbdf68c5834568b1668a02bd2a2458",
      "36eb8f1d09044d2db303c7875ee43cd5",
      "d489f098af9c44098c568b74e5692c86",
      "3cbed714481b4c398756553e3afef83d",
      "0563103763904313926092f834a406c0",
      "eec47c57c80449f1a099a5701328892f",
      "b8098c2cd889438a91ebb92eb415e736",
      "b81b4ccb7966462d871c4db717757d8d",
      "b90c99cba5074d8fb886c8bd109755cb",
      "6b74d842582a4622b3b60873cf3afbfe",
      "a83a2f82924a456bac57eec4f63c45a8",
      "7833b19174be4e93a111daa7b8f35588",
      "33514d7a137148bf8ccaef23a36fbc0c",
      "dc1dcacc073c43729d399293de48354e",
      "9c782323dfb14c72b40d186af138d4b9",
      "bd6861b261b943d8a3b0ab0c717c5751",
      "57608c474d2f4e57b1776f70b62c08c7",
      "d2f18d41280b474c97fc176a43e7821c",
      "14da1e8d12454dda8742b659a6378b8f",
      "cb2d270904ed451b89581ece05a85814",
      "f79ff7d5e25a4c77a95b206b7c704254",
      "fa3821936ef447cbaf84efb348bf3bee",
      "89aa830dd6da4400a3fcf1a1a33dfcfe",
      "8190ca88b39c47de9596211558b0a7bd",
      "51e8ff2ccb5e41d58ea42e5211f81aba",
      "8c2ddd840083462f87f07c77c823f677",
      "09b67d5977554533831522a16020e9fc",
      "8c8c1439f46d40ceb42f126ba32442cb",
      "e74ccba16b6f46a1a84c268ff6816e1f",
      "45f81e7f4fb442bda54b0739d3d68fb8",
      "c5fb6a4e3aa2430cabff80ab118cc22b",
      "38e6e1551f2741c58cfead20d70fd79a",
      "35074184a0bd4bd9b117096b99de3875",
      "0e3c06721bd0428598d9ad696c97a141",
      "92f34452da87469abc6c83642f4c3559",
      "fa34c95979ee4df6a78f4d912d9f70aa",
      "12f75aa6422c4a41b9b2e0c562278576",
      "fe277777efe44eeeafd3e44afbd5384b",
      "46d6518a743b406c9c3cce3a8a3c15a9",
      "2df872358e3e48068d065863cf119bd1",
      "473a4317b3fd46bda8ce98b477d1d62e",
      "eb5ba611e981464cac31403bba453d83",
      "7ee5e6af4dac434a9a1c1a59246e347c",
      "2f78ed74101b4cef8d04f79e3ee300c7",
      "cff8ccd04fed4800ba730e0bc8a7e16b",
      "afd6a67af2c64396ad5862376c45c1e7",
      "81b37dc959544f989e50d60bad6dfdf6",
      "898e8ed4a0b94ef5a5084dec2b182fdc",
      "e240bf28a71a482bb7d3355e2a99fbe0",
      "01903685029647a981aa06b00ce8af9c",
      "34412410b04140e6ad1eceae7377e64e",
      "0d43586c8d8c457c9016fdb7f91921ee",
      "ffd22eb65a3941408407f950eb7dd43f",
      "140fc60470e74a3ab329722fcf420c30",
      "b87b197c251e4801b2630f6e3d78d8e4",
      "a3980276308740aba2bfb84ecbf18cf9",
      "38ea0600d4f6415aa66b06f467b6cf63",
      "cb18f90e15754444b6c178928011493e",
      "501a7baedc7a447bb83ab1ecc3e16c6a",
      "c8a7ded80f7b40d69efaada7eff04e83",
      "ec6c94e2e67f47ea85a8efb4ad959de0",
      "8ff3e2df7071413e89ebf9b1f6ee56be",
      "e012fd1b5de54aa7b7c90654e865b621",
      "bc0eb8bc44a2483599da3d3cca25f62d",
      "b56688febc334cd5a70769fe53fc94ad",
      "f5e05b2d1e7f4334a6a0179a646e408a",
      "95a997b2e3fa40efa2dfd69d88a29893",
      "ca50f5d9007643f09eaba7c60a62fdbd",
      "fcee51187add48dfbace2417d46cf782",
      "9f5e890da7ed48b2ac8aecd35209ab0c",
      "731706a6415d415192e9b60fc1de8a83",
      "488116dff17a451d9712a55c5b988ec6",
      "ade75d3d106b4e86b131e84e5d8f0a53",
      "cffaa8da06744a34b48c5c7bf9a52f4e",
      "10b549f952e34c88829cf7f58f3d4060",
      "ef6b06fc1a7d41c49769d0907fb978e1",
      "61d918f6b43f4e31a2b65c48cb7d4320",
      "d85bd5f4d88c415e917f2ddaea618add",
      "05d0c452f5e242789ba9f5852be47d37",
      "6894e483cb8a474eba639ea0c1cc895e",
      "d5254dfa9990402482e69947739b2e62",
      "33c81f6d11fe4dfdba31df0f46e2a235",
      "53064a9853d04545a394aeb333b2c6ee",
      "05606cf928724c8880b57810bc96bb50",
      "a1be13dcd38b47f69c03bde9f8cde524",
      "6c0f8cd7230e445da9e8e6b158a18bd1",
      "cf78b3af402542fcb7c80222378ae861",
      "e697d6098e344e7982471ee8a556e1ca",
      "f585db8aae524739977bddc1ddf8421f",
      "8c80cb9b9d304f969ecf8b542a164ddd",
      "45f698d0c9164ee0b315cce253fc3f7d",
      "88950b07e73e4099bab0c9db1145cb03",
      "83d4305582fb4514869dc2b24ca19d82",
      "dfc3917482d04bc6bd3bee60845d6f58",
      "a09285f491474be78c0cd85465de13b8",
      "533b6c4ed45e4d80b3888d95caff87e8",
      "5a7409dfb9114a61a4e5a2f2e41682fd",
      "1af017916dae42f4b9e89144fc935723",
      "a2ab51c46e404382976b94bb60ab13d9",
      "5c4a3f571a894f228ddfc32c72e4e7bf",
      "e1da1433fc7544968f2859adcd35944c",
      "71943d56a3e54c1f9ba085e379f305b2",
      "af252e45940142c8b6717fba3b597f92",
      "7ec5831e29ae4302a1e293b069eee260",
      "d3adbe98de8341ada056eb4828f02ef5",
      "0abec82a8e59477baadf8db8b8f7df1b",
      "abf39053160d412680b8b20f7734c440",
      "02262f9471544ffcb79c81935e69bce9",
      "6e3b80ec24724701b2d4806b1e939382",
      "22eff10d33ef49e1b2e6102472f9c040",
      "3c2e3912c7074915aae81a96cff2fbda",
      "0ba149ad7e0e461bbdf0a3d2f0a607a9",
      "184c0a115c3247298d05245b7cb09afc",
      "17f96358527048f2a953101df21be628",
      "1ddb0ebd06464596befbcf82d4b9a0e7",
      "0bbd8602c132446ea564e2ae821c472a",
      "9b72cd2d54554e5ab0a2baac89f941df",
      "7c2a1f77e106474c889063d776369d15",
      "fba48f4fe4c14bb183625e618d68506f",
      "0ae46e979012421695f52e28b0e4b3b8",
      "058e95bb8fcd48648b70e017c3dccdb5",
      "c46a97edcb61433293ac56d69d1a7963",
      "e78183c780b744b3bf4259ac0cc7250e",
      "ecd7a2dd0a3848bf89972ad1fb217d11",
      "e7307eaf956644abaf3bbdfceecd91aa",
      "bc7ffa47700e4b4199344e207631f9a0",
      "64a74cc9d40d4236b9440effd361dd00",
      "7664c78992a6485fa95df50322971edf",
      "99e1ac47dec74654a84d9d7c62f10b99",
      "95656cae56944e518d9560294c1b249d",
      "9588714779c7409cac726219bbd97a24",
      "fc88af06dc1a41dbb26c73cec0c06757",
      "d98d06a174444c6bad96a662a6e92e3e",
      "07f3b2444596452c80bba9212ec267c3",
      "ab66fcf44c0b44a39efc96ae05569999",
      "e154e8a8bf554d9287f0dfde5317305c",
      "9ccdb5e4c3904cdcbb06eccef2eaecb4",
      "d445216885f845419ac0f1e411a00118",
      "c7e2b07965aa4b64afe81d7cc602466a",
      "c3c1353633c94c6ea5dc20ad1cd70fc1",
      "729b5017d9dd4aeebd3faaa1b71a8380",
      "f73a0566e1a346219463a9e541b3a9f2",
      "e5ab04f3628340b0ae83681912e5e9cb",
      "af3c9e6c11c446798aea10145c4baf97",
      "9cc8b5db9e274542ac5767ef5fba1bf7",
      "73c5079da5354b07a99ccccc77ed2b4d",
      "ef238c274c4e47d68704d156d1fe4bac",
      "a09aa70dfeda49b4b0fadf0b1943427f",
      "dfb179a918d0426abdfdc33cb3df33dd",
      "2fb7cb55daa04b0383d32755bb97969a",
      "2cc54ce3ac4e4d268bf53beb2a665e4c",
      "0b357f8dc4444d398e23b26685f89baa",
      "d80bb336920c4c02bf11560e7fcc764a",
      "079fbb7621e340a0926064b8d8352905",
      "2940a0dca79c423b860f51e2377106f1",
      "ce0470f9aeba4280afdceec2eeb6ba30",
      "c723b32d2cd940bd9115934ada190263",
      "e9f1ca8be6244f7aaae7e235336bdc8c",
      "a500edf7cca94c91a66eb99bd4f87c1e",
      "29453bbdb5af4b5f9239eb1bcb339854",
      "4c1c99299ce74c58a454a2d6ba1dffc8",
      "21f0a713e39b42e48be943e73f1366ef",
      "5571978d74de4308872c79dd7a26cc3c",
      "8bc9ab9a740f45bbace11654e0438a50",
      "cd7242ccfecb467f9108c8d1510172e6",
      "69a054541f764127b21fbaef313eb345",
      "9216799f8ae14deca444399454c19354",
      "cb431b70d1874644bab014f36418665b",
      "a935e3d1ecc2468aa3cdb8c39c0b3755",
      "dc1accdce4d948349cf5d3527dd7f904",
      "05478608c4f7470c97019149d9d39759",
      "0f10e32c199948d892882a9da1dd0f43",
      "cafedeef607a47bba7dc41472a29cbe2",
      "3321e5eaf8934391a827ca7057588af6",
      "85d4d5a77c1a45959590850ab0d4e4cc",
      "35b809e40889489594d83ef8bd08b4e1",
      "a408f45f3ec54724a287c5d99f3b5062",
      "2c8b8af450f3409ea725f5b49692e8d0",
      "8bdb4c511ebc462b89602d7ceed22ce8",
      "92c9ce7fecdc479da184b5a1efba68e9",
      "ab9fde8916654acdb43c158918ee5709",
      "9352203c354044d6b5957c697c74dba9",
      "51d7af13322144e3a6db8b33c003e608",
      "00c0391f57394fefac3e8c2837dfb415",
      "956ac9632dcf429b9ebca3901824d3b3",
      "bdd5a9c04bc14044bed0c5dc262e9d68",
      "eb29e489c18f4d558004bf6f9f111cc8",
      "ec5e2d9c79b24726aad9ae119dfc5183",
      "f89c4e5ab3b64f7a9da3432b5bb571ca",
      "dfaf4c2b1572424390d2f1447e8505a4",
      "d8786f16dd9f4c31b7c96957013a1aa1",
      "a59ec6e669244ac091ecd19f84dcc61e",
      "fa3cefea804f4df0aebe17a5802baf3d",
      "851af009fd5b482fbab3d356dbbf8f65",
      "ac47dad954594259a3c392992c2fa782",
      "f829c2df1ed14f9b8b973d154758216d",
      "50ad8049107a4e70a5d432933f9a994d",
      "1c4a57639014481a97c3bd2b43140b9b",
      "016a2d64ecfd4cb38fecb286534e7234",
      "181956b83a02437f9820d91e857ac1f6",
      "871bd0e702594bd5868dd41432225140",
      "fcca368006d64e37b5a167c7a4ab9225",
      "0c7bcc1e7a704d23ba353e14c0b6dee2",
      "8ffacdc4e2164b17830c6eb3d0ee2613",
      "45efa4ade00b45899af4b0cc7b535104",
      "0a1a3d14b1d240878f983ba808098272",
      "1f76622dc80d4001bfa032a5ed9579b2",
      "457eafd884944e5e8f87478e4e361ca3",
      "e0aa618f27cb432fbf31ad08758a51e8",
      "0973b600680441b292cfa6c29769ea98",
      "24425e7a65ee49918c5b2ec69cf4ae73",
      "18eb4e45cc5249ffbd8691d5a098ea45",
      "c79da6371af04b2fa1f844276de26fb9",
      "1c465c28823a4478aa36886b59a62f89",
      "4856a8c3208b4d93ac1ab3b53b172d69",
      "b3f300e888514e979e10240dd3875560",
      "6f7f6213b82a4578936520fcd2b62d70",
      "490f1d412ba647aebdf510cade8171e1",
      "d71197638220485cb6eac9626db514e9",
      "232d1e444ac54f26be07b8b5e902efa4",
      "3c398c7c32a44e1cbc5d6b25c8deae4d",
      "d0826487cdb4406a888c18ddcae6406a",
      "954896508fee4994bdc9c5546d5eda1f",
      "873c1e713cb24f7d93c6bb42ffa1fb8c",
      "f2b5629978824980bcf89177c4fd4df9",
      "c85906ffc5ac4c198493aa73df8c4879",
      "e518043e99e34303808fbc3dccdca510",
      "c6afca1c43cd4189b5a81de506c24287",
      "d6fab9b01d1f49e2a843e7fe199db1c1",
      "962fed843d614f298e13f59dadd4181d",
      "abe7e9d265be458e9ddd11e07ea7ca28"
     ]
    },
    "id": "abf03941",
    "outputId": "524d6ea9-6b50-4e68-f312-d12e28c5cab3"
   },
   "outputs": [],
   "source": [
    "\n",
    "def tokenize_batch(caps):\n",
    "    t = tokenizer(caps, padding=True, truncation=True, return_tensors='pt')\n",
    "    return t['input_ids'], t['attention_mask']\n",
    "\n",
    "training_loss_logger = []\n",
    "\n",
    "def train_one_epoch(model, loader, optimizer, device, loss_fn, scaler=None, ss_ratio=0.1, max_steps=None):\n",
    "    model.train(); running=0.0; steps=0\n",
    "    for images, captions in tqdm(loader, leave=False):\n",
    "        images = images.to(device)\n",
    "        input_ids, attn = tokenize_batch(captions)\n",
    "        input_ids, attn = input_ids.to(device), attn.to(device)\n",
    "        bs, L = input_ids.shape\n",
    "\n",
    "        # Scheduled Sampling: build input sequence token by token\n",
    "        target_ids = torch.cat(\n",
    "            (input_ids[:,1:], torch.zeros(bs,1, dtype=torch.long, device=device)), dim=1\n",
    "        )\n",
    "        tokens_in = torch.full_like(input_ids, tokenizer.pad_token_id)\n",
    "\n",
    "        # Start with [CLS] or SOS token\n",
    "        tokens_in[:,0] = input_ids[:,0]\n",
    "\n",
    "        for t in range(1, L):\n",
    "            use_model_pred = (random.random() < ss_ratio)\n",
    "            if use_model_pred and t > 1:\n",
    "                # Predict next token given past tokens\n",
    "                with torch.no_grad():\n",
    "                    preds = model(images, tokens_in[:,:t], padding_mask=attn[:,:t])\n",
    "                    next_token = preds[:,-1,:].argmax(-1)\n",
    "                tokens_in[:,t] = next_token\n",
    "            else:\n",
    "                # Use ground truth\n",
    "                tokens_in[:,t] = input_ids[:,t]\n",
    "\n",
    "        # Forward + Loss\n",
    "        if scaler is not None:\n",
    "            with torch.cuda.amp.autocast():\n",
    "                preds = model(images, tokens_in, padding_mask=attn)\n",
    "                loss = (loss_fn(preds.transpose(1,2), target_ids) * attn).sum() / attn.sum().clamp_min(1.0)\n",
    "            optimizer.zero_grad(); scaler.scale(loss).backward(); scaler.step(optimizer); scaler.update()\n",
    "        else:\n",
    "            preds = model(images, tokens_in, padding_mask=attn)\n",
    "            loss = (loss_fn(preds.transpose(1,2), target_ids) * attn).sum() / attn.sum().clamp_min(1.0)\n",
    "            optimizer.zero_grad(); loss.backward(); optimizer.step()\n",
    "\n",
    "        running += loss.item(); training_loss_logger.append(loss.item()); steps += 1\n",
    "        if max_steps and steps >= max_steps: break\n",
    "\n",
    "    return running/max(1,steps)\n",
    "\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    loss = train_one_epoch(caption_model, train_loader, optimizer, device, loss_fn, scaler, max_steps=200)\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs} - loss: {loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24",
   "metadata": {
    "id": "a1dc8c8d"
   },
   "source": [
    "# Plot Training Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 410
    },
    "id": "9e3dccfc",
    "outputId": "eaf94e19-1268-4b1d-b26e-448f55a016a1"
   },
   "outputs": [],
   "source": [
    "\n",
    "if training_loss_logger:\n",
    "    plt.figure(figsize=(10,4)); plt.plot(training_loss_logger); plt.title(\"Training Loss\"); plt.xlabel(\"Step\"); plt.show()\n",
    "else:\n",
    "    print(\"No logs yet.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26",
   "metadata": {
    "id": "812f436b"
   },
   "source": [
    "# Inference – Greedy & Beam Search"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27",
   "metadata": {
    "id": "8D2PGqw4xjGB"
   },
   "source": [
    "Beam search keeps the top `beam_size` sequences at each step (by cumulative log-probability). It's a middle ground between greedy (k=1) and full search (exponential)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 323
    },
    "id": "4caa2cbd",
    "outputId": "c5b9104b-59f5-433a-ee89-1f8cd4c5e936"
   },
   "outputs": [],
   "source": [
    "\n",
    "def greedy_decode(model, image_tensor, tokenizer, max_len=30, device='cpu'):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        img = image_tensor.unsqueeze(0).to(device)\n",
    "        start_id = tokenizer.cls_token_id or tokenizer.pad_token_id or 0\n",
    "        cur = torch.full((1,1), start_id, dtype=torch.long, device=device)\n",
    "        attn = torch.ones_like(cur)\n",
    "        for _ in range(max_len):\n",
    "            logits = model(img, cur, padding_mask=attn)[:,-1,:]\n",
    "            nxt = logits.argmax(-1, keepdim=True)\n",
    "            cur = torch.cat([cur, nxt], dim=1); attn = torch.ones_like(cur)\n",
    "            if tokenizer.sep_token_id is not None and nxt.item()==tokenizer.sep_token_id: break\n",
    "        return tokenizer.decode(cur.squeeze().tolist(), skip_special_tokens=True)\n",
    "\n",
    "def beam_search_decode(model, image_tensor, tokenizer, max_len=30, beam_size=3, device='cpu'):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        img = image_tensor.unsqueeze(0).to(device)\n",
    "        start_id = tokenizer.cls_token_id or tokenizer.pad_token_id or 0\n",
    "        beams = [(torch.tensor([[start_id]], device=device, dtype=torch.long), 0.0)]\n",
    "        for _ in range(max_len):\n",
    "            new_beams = []\n",
    "            for seq, score in beams:\n",
    "                logits = model(img, seq, padding_mask=torch.ones_like(seq))[:,-1,:]\n",
    "                log_probs = torch.nn.functional.log_softmax(logits, dim=-1).squeeze(0)\n",
    "                topk = torch.topk(log_probs, beam_size)\n",
    "                for idx, lp in zip(topk.indices.tolist(), topk.values.tolist()):\n",
    "                    nxt = torch.tensor([[idx]], device=device, dtype=torch.long)\n",
    "                    new_beams.append((torch.cat([seq, nxt], dim=1), score + lp))\n",
    "            beams = sorted(new_beams, key=lambda x: x[1], reverse=True)[:beam_size]\n",
    "            if tokenizer.sep_token_id is not None and all(b[0][0,-1].item()==tokenizer.sep_token_id for b in beams):\n",
    "                break\n",
    "        best_seq = beams[0][0].squeeze().tolist()\n",
    "        return tokenizer.decode(best_seq, skip_special_tokens=True)\n",
    "\n",
    "# Demo\n",
    "try:\n",
    "    img_batch, cap_batch = next(iter(val_loader))\n",
    "    img0 = img_batch[0]\n",
    "    plt.figure(figsize=(3,3)); out = torchvision.utils.make_grid(img0,1,normalize=True)\n",
    "    plt.imshow(out.numpy().transpose(1,2,0)); plt.axis('off')\n",
    "    print(\"Reference:\", cap_batch[0])\n",
    "    print(\"Greedy:\", greedy_decode(caption_model, img0, tokenizer, device=device))\n",
    "    print(\"Beam (k=3):\", beam_search_decode(caption_model, img0, tokenizer, device=device))\n",
    "except Exception as e:\n",
    "    print(\"Inference demo failed:\", e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29",
   "metadata": {
    "id": "GEOruHn3Qgax"
   },
   "source": [
    "## Verified Answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30",
   "metadata": {
    "id": "_XVgaQzHyYY4"
   },
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "from transformers import AutoModelForCausalLM\n",
    "import math, itertools\n",
    "\n",
    "# COCO class names (torchvision detection returns these integer labels)\n",
    "COCO_INSTANCE_CATEGORY_NAMES = [\n",
    "    '__background__', 'person', 'bicycle', 'car', 'motorcycle', 'airplane', 'bus',\n",
    "    'train', 'truck', 'boat', 'traffic light', 'fire hydrant', 'stop sign',\n",
    "    'parking meter', 'bench', 'bird', 'cat', 'dog', 'horse', 'sheep', 'cow',\n",
    "    'elephant', 'bear', 'zebra', 'giraffe', 'backpack', 'umbrella', 'handbag',\n",
    "    'tie', 'suitcase', 'frisbee', 'skis', 'snowboard', 'sports ball', 'kite',\n",
    "    'baseball bat', 'baseball glove', 'skateboard', 'surfboard', 'tennis racket',\n",
    "    'bottle', 'wine glass', 'cup', 'fork', 'knife', 'spoon', 'bowl', 'banana',\n",
    "    'apple', 'sandwich', 'orange', 'broccoli', 'carrot', 'hot dog', 'pizza',\n",
    "    'donut', 'cake', 'chair', 'couch', 'potted plant', 'bed', 'dining table',\n",
    "    'toilet', 'tv', 'laptop', 'mouse', 'remote', 'keyboard', 'cell phone',\n",
    "    'microwave', 'oven', 'toaster', 'sink', 'refrigerator', 'book', 'clock',\n",
    "    'vase', 'scissors', 'teddy bear', 'hair drier', 'toothbrush'\n",
    "]\n",
    "\n",
    "# Unnormalize an image tensor (the detector expects 0..1 not normalized)\n",
    "MEAN = torch.tensor([0.485, 0.456, 0.406])\n",
    "STD  = torch.tensor([0.229, 0.224, 0.225])\n",
    "\n",
    "def unnormalize_image_tensor(img_norm):\n",
    "    \"\"\"\n",
    "    img_norm: tensor [C,H,W] normalized with MEAN/STD\n",
    "    Returns: tensor [C,H,W] in 0-1 range suitable for torchvision detection models\n",
    "    \"\"\"\n",
    "    img = img_norm.detach().cpu().clone()\n",
    "    img = img * STD[:, None, None] + MEAN[:, None, None]\n",
    "    img = img.clamp(0., 1.)\n",
    "    return img\n",
    "\n",
    "# Object detection helper (Faster-RCNN)\n",
    "def load_detection_model(device='cpu'):\n",
    "    model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
    "    model.to(device).eval()\n",
    "    return model\n",
    "\n",
    "def detect_objects(img_norm_tensor, detection_model, device='cpu', score_thresh=0.4, topk=10):\n",
    "    \"\"\"\n",
    "    img_norm_tensor: [C,H,W] normalized (same transform used for caption model)\n",
    "    returns: list of (label_name, score) pairs\n",
    "    \"\"\"\n",
    "    img = unnormalize_image_tensor(img_norm_tensor)  # 0..1 tensor\n",
    "    img = img.to(device)\n",
    "    with torch.no_grad():\n",
    "        out = detection_model([img])[0]\n",
    "    labels = out['labels'].cpu().tolist()\n",
    "    scores = out['scores'].cpu().tolist()\n",
    "    pairs = [(COCO_INSTANCE_CATEGORY_NAMES[label], score) for label, score in zip(labels, scores)]\n",
    "    # filter by score and deduplicate with max score\n",
    "    filtered = {}\n",
    "    for name, s in pairs:\n",
    "        if s < score_thresh:\n",
    "            continue\n",
    "        if name not in filtered or s > filtered[name]:\n",
    "            filtered[name] = s\n",
    "    # sort by score, return topk\n",
    "    items = sorted(filtered.items(), key=lambda x: -x[1])[:topk]\n",
    "    return items  # e.g. [('person', 0.96), ('tennis racket', 0.78)]\n",
    "\n",
    "# Candidate generation (beam search returning multiple candidates + log-prob scores)\n",
    "def beam_search_candidates(model, image_tensor, tokenizer, beam_size=5, max_len=30, device='cpu'):\n",
    "    \"\"\"\n",
    "    Returns a list of (decoded_text, token_ids, score) sorted by score (desc).\n",
    "    Score is sum log-probs (higher is better).\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        img = image_tensor.unsqueeze(0).to(device)\n",
    "        start_id = tokenizer.cls_token_id or tokenizer.pad_token_id or 0\n",
    "        beams = [(torch.tensor([[start_id]], device=device, dtype=torch.long), 0.0)]  # (seq, score)\n",
    "        completed = []\n",
    "        for _ in range(max_len):\n",
    "            new_beams = []\n",
    "            for seq, score in beams:\n",
    "                logits = model(img, seq, padding_mask=torch.ones_like(seq).to(device))[:, -1, :]  # [1, vocab]\n",
    "                log_probs = F.log_softmax(logits, dim=-1).squeeze(0)  # [vocab]\n",
    "                topk = torch.topk(log_probs, k=beam_size)\n",
    "                for idx, lp in zip(topk.indices.tolist(), topk.values.tolist()):\n",
    "                    nxt = torch.tensor([[idx]], device=device, dtype=torch.long)\n",
    "                    new_seq = torch.cat([seq, nxt], dim=1)\n",
    "                    new_score = score + float(lp)\n",
    "                    # if EOS token reached, add to completed\n",
    "                    if tokenizer.sep_token_id is not None and idx == tokenizer.sep_token_id:\n",
    "                        completed.append((new_seq, new_score))\n",
    "                    else:\n",
    "                        new_beams.append((new_seq, new_score))\n",
    "            # keep top beam_size ongoing beams\n",
    "            beams = sorted(new_beams, key=lambda x: x[1], reverse=True)[:beam_size]\n",
    "            if len(beams) == 0:\n",
    "                break\n",
    "        all_candidates = completed + beams\n",
    "        # decode and return\n",
    "        out = []\n",
    "        for seq, score in sorted(all_candidates, key=lambda x: x[1], reverse=True):\n",
    "            tokens = seq.squeeze().tolist()\n",
    "            txt = tokenizer.decode(tokens, skip_special_tokens=True)\n",
    "            out.append((txt, tokens, score))\n",
    "        return out\n",
    "\n",
    "# LM scoring using a small causal model (distilgpt2) — used to prefer grammatical captions\n",
    "def load_lm(device='cpu', model_name='distilgpt2'):\n",
    "    lm_tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    lm_model = AutoModelForCausalLM.from_pretrained(model_name).to(device).eval()\n",
    "    return lm_tokenizer, lm_model\n",
    "\n",
    "def lm_score_text(lm_tokenizer, lm_model, text, device='cpu'):\n",
    "    \"\"\"\n",
    "    Returns a log-likelihood-style score (higher = better).\n",
    "    We compute negative loss * token_count so bigger = more probable.\n",
    "    \"\"\"\n",
    "    enc = lm_tokenizer(text, return_tensors='pt')\n",
    "    input_ids = enc['input_ids'].to(device)\n",
    "    with torch.no_grad():\n",
    "        # model returns loss if labels provided\n",
    "        out = lm_model(input_ids, labels=input_ids)\n",
    "        # out.loss is mean token loss -> to make comparable multiply by n_tokens\n",
    "        n_tokens = input_ids.shape[1]\n",
    "        neg_nll = -float(out.loss.item()) * n_tokens\n",
    "    return neg_nll\n",
    "\n",
    "# Object-match scoring and hallucination detection (substring matching)\n",
    "def caption_object_overlap_score(caption, detected_items):\n",
    "    \"\"\"\n",
    "    caption: string\n",
    "    detected_items: list of (label, score)\n",
    "    returns: fraction_of_detected_labels_mentioned, list_of_matched_labels, list_of_mentioned_labels\n",
    "    Strategy: for each detected label (like 'tennis racket'), check if any word of that label appears in caption.\n",
    "    Also collect all COCO label tokens that appear in caption (mentioned_labels).\n",
    "    \"\"\"\n",
    "    cap = caption.lower()\n",
    "    detected_names = [name for name, _ in detected_items]\n",
    "    matched = []\n",
    "    for name in detected_names:\n",
    "        # simple substring match (robust enough for two-word labels like 'tennis racket')\n",
    "        if name in cap:\n",
    "            matched.append(name)\n",
    "        else:\n",
    "            # also try word-by-word match (e.g., 'racket' present)\n",
    "            for w in name.split():\n",
    "                if f' {w} ' in f' {cap} ':\n",
    "                    matched.append(name); break\n",
    "    # find any COCO labels mentioned in caption\n",
    "    mentioned = []\n",
    "    for coco_name in COCO_INSTANCE_CATEGORY_NAMES:\n",
    "        if coco_name == '__background__': continue\n",
    "        if coco_name in cap:\n",
    "            mentioned.append(coco_name)\n",
    "        else:\n",
    "            for w in coco_name.split():\n",
    "                if f' {w} ' in f' {cap} ':\n",
    "                    mentioned.append(coco_name); break\n",
    "    frac = len(matched) / max(1, len(detected_names))\n",
    "    return frac, matched, list(set(mentioned))\n",
    "\n",
    "# n-gram repeat removal (deduplicate repeated chunks)\n",
    "def remove_repeated_ngrams(tokens, max_ngram=4):\n",
    "    \"\"\"\n",
    "    tokens: list of strings\n",
    "    Removes consecutive repeated n-grams (e.g. \"side of a side of a side\" -> \"side of a\").\n",
    "    \"\"\"\n",
    "    changed = True\n",
    "    while changed:\n",
    "        changed = False\n",
    "        for n in range(max_ngram, 0, -1):\n",
    "            if len(tokens) < 2*n:\n",
    "                continue\n",
    "            i = 0\n",
    "            while i + 2*n <= len(tokens):\n",
    "                if tokens[i:i+n] == tokens[i+n:i+2*n]:\n",
    "                    # remove the second repeated ngram\n",
    "                    del tokens[i+n:i+2*n]\n",
    "                    changed = True\n",
    "                    # do NOT advance i to recheck for triple repeats\n",
    "                else:\n",
    "                    i += 1\n",
    "            if changed:\n",
    "                break\n",
    "    return tokens\n",
    "\n",
    "def dedupe_and_postprocess_caption(raw_caption):\n",
    "    toks = raw_caption.strip().split()\n",
    "    toks = remove_repeated_ngrams(toks, max_ngram=4)\n",
    "    # collapse exact token duplicates e.g. 'a a a' -> 'a'\n",
    "    cleaned = []\n",
    "    for t in toks:\n",
    "        if len(cleaned) > 0 and cleaned[-1] == t:\n",
    "            continue\n",
    "        cleaned.append(t)\n",
    "    return \" \".join(cleaned)\n",
    "\n",
    "# High-level verified caption generator\n",
    "def generate_verified_caption(image_tensor,\n",
    "                              caption_model,\n",
    "                              caption_tokenizer,\n",
    "                              detection_model=None,\n",
    "                              lm_tokenizer=None,\n",
    "                              lm_model=None,\n",
    "                              device='cpu',\n",
    "                              beam_k=5,\n",
    "                              alpha_lm=1.0,\n",
    "                              beta_overlap=2.0,\n",
    "                              gamma_halluc=1.0):\n",
    "    \"\"\"\n",
    "    image_tensor: single normalized image [C,H,W] (same preprocessing used for caption_model)\n",
    "    caption_model: your encoder-decoder caption model (callable that accepts (img, seq, padding_mask))\n",
    "    caption_tokenizer: tokenizer used by your caption decoder (for decode)\n",
    "    detection_model: torchvision detection model (optional but recommended)\n",
    "    lm_tokenizer, lm_model: small causal LM for grammar scoring (optional)\n",
    "    Returns: best_caption, diagnostics dict\n",
    "    \"\"\"\n",
    "    # 1) detect objects\n",
    "    detected = []\n",
    "    if detection_model is not None:\n",
    "        detected = detect_objects(image_tensor, detection_model, device=device, score_thresh=0.35, topk=15)\n",
    "    # 2) generate beam candidates\n",
    "    candidates = beam_search_candidates(caption_model, image_tensor, caption_tokenizer, beam_size=beam_k, max_len=28, device=device)\n",
    "    scored = []\n",
    "    for txt, token_ids, score in candidates:\n",
    "        # LM score (higher better)\n",
    "        if lm_model is not None:\n",
    "            try:\n",
    "                lm_s = lm_score_text(lm_tokenizer, lm_model, txt, device=device)\n",
    "            except Exception as e:\n",
    "                lm_s = 0.0\n",
    "        else:\n",
    "            lm_s = 0.0\n",
    "        # overlap/hallucination\n",
    "        frac_overlap, matched, mentioned = caption_object_overlap_score(txt, detected)\n",
    "        hallucinated_labels = [m for m in mentioned if m not in matched]\n",
    "        # combined score: weighted sum (tune alpha/beta/gamma)\n",
    "        combined = alpha_lm * lm_s + beta_overlap * (frac_overlap) - gamma_halluc * len(hallucinated_labels)\n",
    "        scored.append({\n",
    "            'text': txt,\n",
    "            'tokens': token_ids,\n",
    "            'beam_score': score,\n",
    "            'lm_score': lm_s,\n",
    "            'frac_overlap': frac_overlap,\n",
    "            'matched': matched,\n",
    "            'mentioned': mentioned,\n",
    "            'hallucinated': hallucinated_labels,\n",
    "            'combined': combined\n",
    "        })\n",
    "    # pick best by combined score\n",
    "    if len(scored) == 0:\n",
    "        return \"\", {'error': 'no candidates'}\n",
    "    best = sorted(scored, key=lambda x: x['combined'], reverse=True)[0]\n",
    "    best_text = dedupe_and_postprocess_caption(best['text'])\n",
    "    best['final_text'] = best_text\n",
    "    best['detected'] = detected\n",
    "    return best_text, best\n",
    "\n",
    "# ---------------------\n",
    "# Example usage\n",
    "# ---------------------\n",
    "# 1) load detector and LM once\n",
    "detection_model = load_detection_model(device=device)   # recommended\n",
    "lm_tokenizer, lm_model = load_lm(device=device)         # optional (distilgpt2)\n",
    "\n",
    "caption, diag = generate_verified_caption(img0, caption_model, tokenizer,\n",
    "                                          detection_model=detection_model,\n",
    "                                          lm_tokenizer=lm_tokenizer, lm_model=lm_model,\n",
    "                                          device=device, beam_k=5)\n",
    "print(\"Detected objects:\", diag['detected'])\n",
    "print(\"Matched labels in caption:\", diag['matched'])\n",
    "print(\"Hallucinated labels:\", diag['hallucinated'])\n",
    "print(\"Returned caption:\", caption)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31",
   "metadata": {
    "id": "833e32f4"
   },
   "source": [
    "# Evaluation – BLEU & CIDEr (optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f3FjX2Hnchha",
    "outputId": "eecacd2a-2580-410a-ae8e-151061f4e504"
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"punkt_tab\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 234,
     "referenced_widgets": [
      "fc16519705b245fd9b64028cd6e412f3",
      "c47e3a4ce8e5447db5233e56769c3212",
      "f78cd3ff76ab41e293d9876a3632d688",
      "33653ab5ec7e40479552f18267f03764",
      "c8272d8ffeba44dd8ca896791e1688e8",
      "48784e294d2046d5bda5dd6d5fc080ab",
      "c7966bbeda9245a093e5546824cfc914",
      "f77a482ec76241a7bc186794b08bc832",
      "27daca7ddb5c458b8eb7a8b90994d7b8",
      "684593aae5a442cfaa4db31eb73476f1",
      "3f12abf19cd542e18edce5ce63fff57a",
      "267a825d105e4fe3b53135dab0f1c21f",
      "ed2e0e622c3d440e943e5a2d55910f5b",
      "60db474b61a24d1bb1b98852666f2c27",
      "54d0965fe77949a985eec6a3720aefd9",
      "6e773e17ec4842f3a6fb7af983803bec",
      "63de4c39b818431ea4f07baf40496d67",
      "25ed3278bf734b97959e37d861bfa5e2",
      "65832d688c644188a91fee5db774e203",
      "2392f337fa4e4c2ab1f9d52e57e1c9cc",
      "6240fb92ab52433081407fda66848a35",
      "ac388b07c3ee4ce68d9569d34b9139a4"
     ]
    },
    "id": "c1bb7aa8",
    "outputId": "d55f926f-5ca3-4985-9d68-28c94a09182b"
   },
   "outputs": [],
   "source": [
    "\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "from pycocotools.coco import COCO\n",
    "from pycocoevalcap.cider.cider import Cider\n",
    "\n",
    "def evaluate_bleu(loader, model, tokenizer, max_samples=100):\n",
    "    model.eval()\n",
    "    smoothie = SmoothingFunction().method4\n",
    "    scores = []; n = 0\n",
    "    for imgs, caps in tqdm(loader, leave=False):\n",
    "        for i in range(len(imgs)):\n",
    "            pred = greedy_decode(model, imgs[i], tokenizer, device=device)\n",
    "            ref = caps[i]\n",
    "            ref_tokens = nltk.word_tokenize(ref.lower())\n",
    "            hyp_tokens = nltk.word_tokenize(pred.lower())\n",
    "            scores.append(sentence_bleu([ref_tokens], hyp_tokens, smoothing_function=smoothie))\n",
    "            n += 1\n",
    "            if n >= max_samples: break\n",
    "        if n >= max_samples: break\n",
    "    return float(np.mean(scores)) if scores else 0.0\n",
    "\n",
    "def evaluate_cider(val_ann_path, ids_to_caps_pred, max_samples=200):\n",
    "    coco = COCO(str(val_ann_path))\n",
    "    gts = {}\n",
    "    res = {}\n",
    "    count = 0\n",
    "\n",
    "    for img_id, pred in ids_to_caps_pred.items():\n",
    "        ann_ids = coco.getAnnIds(imgIds=img_id)\n",
    "        anns = coco.loadAnns(ann_ids)\n",
    "\n",
    "        # keep only caption strings\n",
    "        gts[img_id] = [a[\"caption\"] for a in anns]\n",
    "        res[img_id] = [pred]\n",
    "\n",
    "        count += 1\n",
    "        if count >= max_samples:\n",
    "            break\n",
    "\n",
    "    cider_scorer = Cider()\n",
    "    score, _ = cider_scorer.compute_score(gts, res)\n",
    "    return float(score)\n",
    "\n",
    "\n",
    "\n",
    "# BLEU quick check\n",
    "try:\n",
    "    bleu = evaluate_bleu(val_loader, caption_model, tokenizer, max_samples=100)\n",
    "    print(f\"BLEU (greedy, 100 samples): {bleu:.3f}\")\n",
    "except Exception as e:\n",
    "    print(\"BLEU eval failed:\", e)\n",
    "\n",
    "# CIDEr subset on first 200 images\n",
    "try:\n",
    "    coco_val = COCO(str(val_ann))\n",
    "    img_ids = coco_val.getImgIds()[:200]\n",
    "    ids_to_pred = {}\n",
    "    from PIL import Image\n",
    "    for img_id in tqdm(img_ids, leave=False):\n",
    "        info = coco_val.loadImgs([img_id])[0]\n",
    "        img_path = (val_images / info['file_name']).as_posix()\n",
    "        img = Image.open(img_path).convert('RGB')\n",
    "        img_t = val_transform(img)\n",
    "        pred = greedy_decode(caption_model, img_t, tokenizer, device=device)\n",
    "        ids_to_pred[img_id] = pred\n",
    "\n",
    "    cider = evaluate_cider(val_ann, ids_to_pred, max_samples=200)\n",
    "    print(f\"CIDEr (subset): {cider:.3f}\")\n",
    "except Exception as e:\n",
    "    print(\"CIDEr eval failed:\", e)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34",
   "metadata": {
    "id": "3c61b4d3"
   },
   "source": [
    "# Save/Load Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35",
   "metadata": {
    "id": "92d9ccda"
   },
   "outputs": [],
   "source": [
    "\n",
    "def save_model(model, path='/content/caption_model.pth'):\n",
    "    torch.save(model.state_dict(), path); print(\"Saved:\", path)\n",
    "def load_model(model, path='/content/caption_model.pth', map_location=None):\n",
    "    model.load_state_dict(torch.load(path, map_location=map_location)); print(\"Loaded:\", path)\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "GEOruHn3Qgax",
    "833e32f4"
   ],
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
